{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9068dc",
   "metadata": {},
   "source": [
    "### A regression problem\n",
    "\n",
    "We want to predict y for x in a sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fcc98a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1600, Validation samples: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/82/dc_jy61x0wsbn9lnnxghrp780000gn/T/ipykernel_57527/2046488145.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(x_train).float(), torch.tensor(y_train).float())\n",
      "/var/folders/82/dc_jy61x0wsbn9lnnxghrp780000gn/T/ipykernel_57527/2046488145.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_dataset = TensorDataset(torch.tensor(x_val).float(), torch.tensor(y_val).float())\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Generating a sine wave with noise\n",
    "x = torch.linspace(0, 2 * torch.pi, 2000).view(-1, 1)  # 1000 samples\n",
    "y = torch.sin(x) + 0.1 * torch.randn(x.size())         # Add noise\n",
    "\n",
    "# Task: Split into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Wrap the datasets in PyTorch DataLoaders\n",
    "train_dataset = TensorDataset(torch.tensor(x_train).float(), torch.tensor(y_train).float())\n",
    "val_dataset = TensorDataset(torch.tensor(x_val).float(), torch.tensor(y_val).float())\n",
    "\n",
    "# Leave batch size for tuning later\n",
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df23203",
   "metadata": {},
   "source": [
    "#### This is our deep Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1664acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeeperNN(\n",
      "  (hidden1): Linear(in_features=1, out_features=128, bias=True)\n",
      "  (hidden2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (hidden3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (output): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define a deeper feedforward neural network\n",
    "class DeeperNN(nn.Module):\n",
    "    def __init__(self, hidden_units=128):\n",
    "        super(DeeperNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(1, hidden_units)\n",
    "        self.hidden2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.hidden3 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.output = nn.Linear(hidden_units, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = torch.relu(self.hidden3(x))\n",
    "        return self.output(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = DeeperNN(hidden_units=128)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688d9f73",
   "metadata": {},
   "source": [
    "#### Here we define basic a training function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba2ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            predictions = model(batch_x)\n",
    "            val_loss += criterion(predictions, batch_y).item()\n",
    "    return val_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e26d1",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Expand the hyperparameters for Grid Search (directly below). \n",
    "The more values you add, the bigger the search space will be - this is usually good practice, as you don't want to get stuck in local minima but rather find the global minumum -> The hyperparameter combination with the least validation loss. \n",
    "\n",
    "\n",
    "![Local Minimum](https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Extrema_example_original.svg/500px-Extrema_example_original.svg.png)\n",
    "\n",
    "\n",
    "\n",
    "Run the cell after you expanded the hyperparameter sets by as many values as you think are enough. The training might take a few minutes. \n",
    "\n",
    "\n",
    "\n",
    "Compare the results with the Random Search results one block below Grid Search. What did you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8404857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for grid search\n",
    "hidden_units_grid = [8, 16, 32, 64, 128, 256]\n",
    "learning_rates = [1, 0.1, 0.001]\n",
    "batch_sizes = [2, 4, 8, 16, 32]\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d31739",
   "metadata": {},
   "source": [
    "Our Grid Search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10eb4e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_units=8, lr=1, batch_size=2, val_loss=0.5614\n",
      "hidden_units=8, lr=1, batch_size=4, val_loss=0.5555\n",
      "hidden_units=8, lr=1, batch_size=8, val_loss=0.5607\n",
      "hidden_units=8, lr=1, batch_size=16, val_loss=0.5721\n",
      "hidden_units=8, lr=1, batch_size=32, val_loss=0.5550\n",
      "hidden_units=8, lr=0.1, batch_size=2, val_loss=0.5774\n",
      "hidden_units=8, lr=0.1, batch_size=4, val_loss=0.0609\n",
      "hidden_units=8, lr=0.1, batch_size=8, val_loss=0.0747\n",
      "hidden_units=8, lr=0.1, batch_size=16, val_loss=0.0128\n",
      "hidden_units=8, lr=0.1, batch_size=32, val_loss=0.0227\n",
      "hidden_units=8, lr=0.001, batch_size=2, val_loss=0.0116\n",
      "hidden_units=8, lr=0.001, batch_size=4, val_loss=0.0120\n",
      "hidden_units=8, lr=0.001, batch_size=8, val_loss=0.0116\n",
      "hidden_units=8, lr=0.001, batch_size=16, val_loss=0.0135\n",
      "hidden_units=8, lr=0.001, batch_size=32, val_loss=0.0283\n",
      "hidden_units=16, lr=1, batch_size=2, val_loss=0.5742\n",
      "hidden_units=16, lr=1, batch_size=4, val_loss=0.5498\n",
      "hidden_units=16, lr=1, batch_size=8, val_loss=0.6383\n",
      "hidden_units=16, lr=1, batch_size=16, val_loss=0.5511\n",
      "hidden_units=16, lr=1, batch_size=32, val_loss=0.5548\n",
      "hidden_units=16, lr=0.1, batch_size=2, val_loss=0.5510\n",
      "hidden_units=16, lr=0.1, batch_size=4, val_loss=0.2005\n",
      "hidden_units=16, lr=0.1, batch_size=8, val_loss=0.0565\n",
      "hidden_units=16, lr=0.1, batch_size=16, val_loss=0.0400\n",
      "hidden_units=16, lr=0.1, batch_size=32, val_loss=0.0522\n",
      "hidden_units=16, lr=0.001, batch_size=2, val_loss=0.0113\n",
      "hidden_units=16, lr=0.001, batch_size=4, val_loss=0.0124\n",
      "hidden_units=16, lr=0.001, batch_size=8, val_loss=0.0121\n",
      "hidden_units=16, lr=0.001, batch_size=16, val_loss=0.0172\n",
      "hidden_units=16, lr=0.001, batch_size=32, val_loss=0.0121\n",
      "hidden_units=32, lr=1, batch_size=2, val_loss=0.6932\n",
      "hidden_units=32, lr=1, batch_size=4, val_loss=0.5512\n",
      "hidden_units=32, lr=1, batch_size=8, val_loss=0.5497\n",
      "hidden_units=32, lr=1, batch_size=16, val_loss=0.5497\n",
      "hidden_units=32, lr=1, batch_size=32, val_loss=1.2178\n",
      "hidden_units=32, lr=0.1, batch_size=2, val_loss=0.5749\n",
      "hidden_units=32, lr=0.1, batch_size=4, val_loss=0.5550\n",
      "hidden_units=32, lr=0.1, batch_size=8, val_loss=0.0659\n",
      "hidden_units=32, lr=0.1, batch_size=16, val_loss=0.0457\n",
      "hidden_units=32, lr=0.1, batch_size=32, val_loss=0.0123\n",
      "hidden_units=32, lr=0.001, batch_size=2, val_loss=0.0116\n",
      "hidden_units=32, lr=0.001, batch_size=4, val_loss=0.0134\n",
      "hidden_units=32, lr=0.001, batch_size=8, val_loss=0.0115\n",
      "hidden_units=32, lr=0.001, batch_size=16, val_loss=0.0113\n",
      "hidden_units=32, lr=0.001, batch_size=32, val_loss=0.0146\n",
      "hidden_units=64, lr=1, batch_size=2, val_loss=0.8556\n",
      "hidden_units=64, lr=1, batch_size=4, val_loss=0.5498\n",
      "hidden_units=64, lr=1, batch_size=8, val_loss=0.5526\n",
      "hidden_units=64, lr=1, batch_size=16, val_loss=0.9445\n",
      "hidden_units=64, lr=1, batch_size=32, val_loss=0.5582\n",
      "hidden_units=64, lr=0.1, batch_size=2, val_loss=0.5774\n",
      "hidden_units=64, lr=0.1, batch_size=4, val_loss=0.5600\n",
      "hidden_units=64, lr=0.1, batch_size=8, val_loss=0.5817\n",
      "hidden_units=64, lr=0.1, batch_size=16, val_loss=0.0176\n",
      "hidden_units=64, lr=0.1, batch_size=32, val_loss=0.0239\n",
      "hidden_units=64, lr=0.001, batch_size=2, val_loss=0.0151\n",
      "hidden_units=64, lr=0.001, batch_size=4, val_loss=0.0117\n",
      "hidden_units=64, lr=0.001, batch_size=8, val_loss=0.0129\n",
      "hidden_units=64, lr=0.001, batch_size=16, val_loss=0.0161\n",
      "hidden_units=64, lr=0.001, batch_size=32, val_loss=0.0131\n",
      "hidden_units=128, lr=1, batch_size=2, val_loss=0.5730\n",
      "hidden_units=128, lr=1, batch_size=4, val_loss=0.7714\n",
      "hidden_units=128, lr=1, batch_size=8, val_loss=0.7702\n",
      "hidden_units=128, lr=1, batch_size=16, val_loss=0.5500\n",
      "hidden_units=128, lr=1, batch_size=32, val_loss=9.3636\n",
      "hidden_units=128, lr=0.1, batch_size=2, val_loss=0.5557\n",
      "hidden_units=128, lr=0.1, batch_size=4, val_loss=0.5568\n",
      "hidden_units=128, lr=0.1, batch_size=8, val_loss=0.0177\n",
      "hidden_units=128, lr=0.1, batch_size=16, val_loss=0.0261\n",
      "hidden_units=128, lr=0.1, batch_size=32, val_loss=0.0195\n",
      "hidden_units=128, lr=0.001, batch_size=2, val_loss=0.0213\n",
      "hidden_units=128, lr=0.001, batch_size=4, val_loss=0.0114\n",
      "hidden_units=128, lr=0.001, batch_size=8, val_loss=0.0129\n",
      "hidden_units=128, lr=0.001, batch_size=16, val_loss=0.0115\n",
      "hidden_units=128, lr=0.001, batch_size=32, val_loss=0.0118\n",
      "hidden_units=256, lr=1, batch_size=2, val_loss=0.5672\n",
      "hidden_units=256, lr=1, batch_size=4, val_loss=0.6213\n",
      "hidden_units=256, lr=1, batch_size=8, val_loss=1554.8317\n",
      "hidden_units=256, lr=1, batch_size=16, val_loss=88.3654\n",
      "hidden_units=256, lr=1, batch_size=32, val_loss=1.3973\n",
      "hidden_units=256, lr=0.1, batch_size=2, val_loss=0.5512\n",
      "hidden_units=256, lr=0.1, batch_size=4, val_loss=0.1117\n",
      "hidden_units=256, lr=0.1, batch_size=8, val_loss=0.5510\n",
      "hidden_units=256, lr=0.1, batch_size=16, val_loss=0.0522\n",
      "hidden_units=256, lr=0.1, batch_size=32, val_loss=0.0189\n",
      "hidden_units=256, lr=0.001, batch_size=2, val_loss=0.0215\n",
      "hidden_units=256, lr=0.001, batch_size=4, val_loss=0.0120\n",
      "hidden_units=256, lr=0.001, batch_size=8, val_loss=0.0192\n",
      "hidden_units=256, lr=0.001, batch_size=16, val_loss=0.0140\n",
      "hidden_units=256, lr=0.001, batch_size=32, val_loss=0.0128\n",
      "Best Grid Search Params: hidden_units=32, lr=0.001, batch_size=16 with val_loss=0.0113\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Grid Search iterates over all possible hyperparameter combinations\n",
    "for hidden_units in hidden_units_grid:\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            \n",
    "            # Prepare DataLoader\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "            # Initialize model, criterion, and optimizer\n",
    "            model = DeeperNN(hidden_units=hidden_units)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            # Train the model and get validation loss\n",
    "            val_loss = train_model(model, criterion, optimizer, train_loader, val_loader, epochs)\n",
    "            print(f\"hidden_units={hidden_units}, lr={lr}, batch_size={batch_size}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "            # Track the best configuration\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_params = (hidden_units, lr, batch_size)\n",
    "\n",
    "print(f\"Best Grid Search Params: hidden_units={best_params[0]}, lr={best_params[1]}, batch_size={best_params[2]} with val_loss={best_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6bcfc",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "\n",
    "We will use the same parameters you defined in the first task. However we won't explore every possible combination, but take random combinations for a number of trials - you can tweak the number of trials if you like, however this is not necessary. \n",
    "\n",
    "# Task 1.1\n",
    "\n",
    "Implement an algorithm that will randomly chose values from the sets you defined above. \n",
    "\n",
    "Hint: Check the import statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63047146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_units=128, lr=1, batch_size=32, val_loss=17.6688\n",
      "hidden_units=128, lr=0.001, batch_size=2, val_loss=0.0139\n",
      "hidden_units=64, lr=0.001, batch_size=2, val_loss=0.0138\n",
      "hidden_units=64, lr=1, batch_size=16, val_loss=0.5496\n",
      "hidden_units=64, lr=0.1, batch_size=4, val_loss=0.1012\n",
      "hidden_units=64, lr=0.001, batch_size=16, val_loss=0.0114\n",
      "hidden_units=64, lr=0.1, batch_size=2, val_loss=0.5616\n",
      "hidden_units=128, lr=0.001, batch_size=32, val_loss=0.0121\n",
      "hidden_units=16, lr=0.1, batch_size=2, val_loss=0.5497\n",
      "hidden_units=32, lr=0.1, batch_size=8, val_loss=0.0724\n",
      "Best Random Search Params: hidden_units=64, lr=0.001, batch_size=16 with val_loss=0.0114\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Random Search\n",
    "num_trials = 10\n",
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "\n",
    "for _ in range(num_trials):\n",
    "    # Randomly sample hyperparameters\n",
    "    hidden_units = random.choice(hidden_units_grid)\n",
    "    lr = random.choice(learning_rates)\n",
    "    batch_size = random.choice(batch_sizes)\n",
    "\n",
    "    # Prepare DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = DeeperNN(hidden_units=hidden_units)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train the model and get validation loss\n",
    "    val_loss = train_model(model, criterion, optimizer, train_loader, val_loader, epochs)\n",
    "    print(f\"hidden_units={hidden_units}, lr={lr}, batch_size={batch_size}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "    # Track the best configuration\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_params = (hidden_units, lr, batch_size)\n",
    "\n",
    "print(f\"Best Random Search Params: hidden_units={best_params[0]}, lr={best_params[1]}, batch_size={best_params[2]} with val_loss={best_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dc04ba",
   "metadata": {},
   "source": [
    "#### What did you notice? What are key differences? \n",
    "\n",
    "Put your answers here. \n",
    "\n",
    "#### Even if results may be better by a small margin, what makes Grid Search unpractial compared to Random Search?\n",
    "\n",
    "Hint: Think about the relation between the number of Hyperparameters and the number of combinations that result from that. Especially for harder problems with more dimensions. \n",
    "\n",
    "Put your answers here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf3f41e",
   "metadata": {},
   "source": [
    "## Evolutionary Algorithms\n",
    "\n",
    "The following codeblock is a basic implementation of an Evolutionary Algorithm. Go over it and try to understand what happens. Remember what was told in the theoretical part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ae47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "def evaluate_model(model, train_loader, val_loader, learning_rate, epochs):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# Perform mutation on hyperparameters\n",
    "def mutate(params, mutation_rate=0.1):\n",
    "    new_params = params.copy()\n",
    "    if random.random() < mutation_rate:\n",
    "        if random.random() < 0.5:\n",
    "            new_params['hidden_units'] = random.randint(4, 128)  # Choose hidden units randomly between 4 and 128\n",
    "            new_params['learning_rate'] = random.uniform(0.0001, 1)  # Choose learning rate randomly in range\n",
    "            new_params['batch_size'] = random.randint(2, 128)  # Choose batch size between 2 and 128\n",
    "            new_params['epochs'] = random.randint(10, 100)  # Mutate epochs within a range\n",
    "        else:\n",
    "            new_params['hidden_units'] += random.randint(-8, 8)  # Choose hidden units randomly between 4 and 128\n",
    "            new_params['learning_rate'] += random.uniform(-0.001, 0.001)  # Choose learning rate randomly in range\n",
    "            new_params['batch_size'] += random.randint(-8, 8)  # Choose batch size between 2 and 128\n",
    "            new_params['epochs'] += random.randint(-8, 8)  # Mutate epochs within a range\n",
    "    return new_params\n",
    "\n",
    "# Perform crossover between two sets of hyperparameters\n",
    "def crossover(parent1, parent2):\n",
    "    child = {}\n",
    "    for key in parent1:\n",
    "        child[key] = parent1[key] if random.random() > 0.5 else parent2[key]\n",
    "    return child\n",
    "\n",
    "# Main evolutionary algorithm\n",
    "def evolutionary_algorithm(num_generations=20, population_size=10, mutation_rate=0.5):\n",
    "    # Initialize population with random hyperparameters\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        params = {\n",
    "            'hidden_units': random.randint(4, 1024),  # Randomly chosen hidden_units\n",
    "            'learning_rate': random.uniform(0.0001, 1),  # Randomly chosen learning_rate\n",
    "            'batch_size': random.randint(2, 1048),  # Randomly chosen batch_size\n",
    "            'epochs': random.randint(1, 50)  # Randomly chosen epochs\n",
    "        }\n",
    "        population.append(params)\n",
    "\n",
    "    best_hyperparams = None\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for generation in range(num_generations):\n",
    "        # Evaluate fitness of each set of hyperparameters in the population\n",
    "        fitness = []\n",
    "        for params in population:\n",
    "            # Create DataLoaders with the specified batch size\n",
    "            train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=params['batch_size'])\n",
    "\n",
    "            # Initialize a new model with the current hyperparameters\n",
    "            model = DeeperNN(hidden_units=params['hidden_units'])\n",
    "\n",
    "            # Evaluate the model's validation loss\n",
    "            val_loss = evaluate_model(model, train_loader, val_loader, params['learning_rate'], params['epochs'])\n",
    "            fitness.append((val_loss, params))\n",
    "\n",
    "            # Update the best hyperparameters if the current model is better\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_hyperparams = params\n",
    "\n",
    "        # Sort population based on fitness (lower loss = better fitness)\n",
    "        fitness.sort(key=lambda x: x[0])\n",
    "        population = [individual[1] for individual in fitness]\n",
    "\n",
    "        # Print the best fitness in the current generation\n",
    "        print(f\"Generation {generation + 1}: Best Validation Loss = {fitness[0][0]:.6f}\")\n",
    "\n",
    "        # Elitism: Keep the top 2 hyperparameter sets unchanged\n",
    "        next_population = population[:2]\n",
    "\n",
    "        # Generate the rest of the population via crossover and mutation\n",
    "        while len(next_population) < population_size:\n",
    "            parent1 = random.choice(population[:population_size // 2])\n",
    "            parent2 = random.choice(population[:population_size // 2])\n",
    "            child = crossover(parent1, parent2)\n",
    "            child = mutate(child, mutation_rate=mutation_rate)  # Mutate the child's hyperparameters\n",
    "            next_population.append(child)\n",
    "\n",
    "        # Update the population\n",
    "        population = next_population\n",
    "\n",
    "    return best_hyperparams, best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e6160",
   "metadata": {},
   "source": [
    "# Task 2: Evolutionary Algorithms\n",
    "\n",
    "![Local Minimum](https://www.americanscientist.org/sites/americanscientist.org/files/20144141249210337-2014-05TechnologueFp170.jpg)\n",
    "\n",
    "Now this one is a bit more tricky! We now want to use a Evolutionary Algorithm to find good Hyperparameters. Remember the theory behind selections. \n",
    "\n",
    "\n",
    "Run the algorithm with the default values first. What do you notice? \n",
    "\n",
    "If your machine is not that powerful training might take more than a few minutes. In that case you can stop the calculations after few generations. \n",
    "\n",
    "- Remember what the mutation rate does? Maybe try tweaking this value first in a useful manner. Hint: Check the mutation function. Write down why the default value is maybe not the best fit. \n",
    "\n",
    "- Maybe you want to check the population size next. Check how the algorithm choses the parents to influence the next generations population in the code above and tweak the value. So, why is the default population size of 4 maybe not the best choice? \n",
    "\n",
    "- Alright, step by step in the right direction. Maybe it makes sense to increase the number of generations? Check the comic in respect to the randomness in mutations to understand why ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97193b",
   "metadata": {},
   "source": [
    "Here is space for your notes and answers.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03c29f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1: Best Validation Loss = 0.012560\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m population_size = \u001b[32m8\u001b[39m\n\u001b[32m      5\u001b[39m mutation_rate = \u001b[32m0.2\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m best_hyperparams, best_loss = \u001b[43mevolutionary_algorithm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_generations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_generations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmutation_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmutation_rate\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Print the best hyperparameters and corresponding loss\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest Hyperparameters:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mevolutionary_algorithm\u001b[39m\u001b[34m(num_generations, population_size, mutation_rate)\u001b[39m\n\u001b[32m     99\u001b[39m     parent2 = random.choice(population[:population_size // \u001b[32m2\u001b[39m])\n\u001b[32m    100\u001b[39m     child = crossover(parent1, parent2)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     child = \u001b[43mmutate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutation_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmutation_rate\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Mutate the child's hyperparameters\u001b[39;00m\n\u001b[32m    102\u001b[39m     next_population.append(child)\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Update the population\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mmutate\u001b[39m\u001b[34m(params, mutation_rate)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     37\u001b[39m     new_params[\u001b[33m'\u001b[39m\u001b[33mhidden_units\u001b[39m\u001b[33m'\u001b[39m] += random.randint(-\u001b[32m8\u001b[39m, \u001b[32m8\u001b[39m)  \u001b[38;5;66;03m# Choose hidden units randomly between 4 and 128\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     new_params[\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m] += \u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Choose learning rate randomly in range\u001b[39;00m\n\u001b[32m     39\u001b[39m     new_params[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m] += random.randint(-\u001b[32m8\u001b[39m, \u001b[32m8\u001b[39m)  \u001b[38;5;66;03m# Choose batch size between 2 and 128\u001b[39;00m\n\u001b[32m     40\u001b[39m     new_params[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m] += random.randint(-\u001b[32m8\u001b[39m, \u001b[32m8\u001b[39m)  \u001b[38;5;66;03m# Mutate epochs within a range\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/random.py:340\u001b[39m, in \u001b[36mRandom.randint\u001b[39m\u001b[34m(self, a, b)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrandint\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, b):\n\u001b[32m    337\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return random integer in range [a, b], including both end points.\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/random.py:305\u001b[39m, in \u001b[36mRandom.randrange\u001b[39m\u001b[34m(self, start, stop, step)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Choose a random item from range(stop) or range(start, stop[, step]).\u001b[39;00m\n\u001b[32m    297\u001b[39m \n\u001b[32m    298\u001b[39m \u001b[33;03mRoughly equivalent to ``choice(range(start, stop, step))`` but\u001b[39;00m\n\u001b[32m    299\u001b[39m \u001b[33;03msupports arbitrarily large ranges and is optimized for common cases.\u001b[39;00m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[38;5;66;03m# This code is a bit messy to make it fast for the\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[38;5;66;03m# common case while still doing adequate error checking.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m istart = \u001b[43m_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    307\u001b[39m     \u001b[38;5;66;03m# We don't check for \"step != 1\" because it hasn't been\u001b[39;00m\n\u001b[32m    308\u001b[39m     \u001b[38;5;66;03m# type checked and converted to an integer yet.\u001b[39;00m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ONE:\n",
      "\u001b[31mTypeError\u001b[39m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Run the evolutionary algorithm\n",
    "if __name__ == \"__main__\":\n",
    "    num_generations = 10\n",
    "    population_size = 8\n",
    "    mutation_rate = 0.2\n",
    "\n",
    "    best_hyperparams, best_loss = evolutionary_algorithm(\n",
    "        num_generations=num_generations, \n",
    "        population_size=population_size,\n",
    "        mutation_rate=mutation_rate\n",
    "    )\n",
    "\n",
    "    # Print the best hyperparameters and corresponding loss\n",
    "    print(\"\\nBest Hyperparameters:\")\n",
    "    print(f\"Hidden Units: {best_hyperparams['hidden_units']}\")\n",
    "    print(f\"Learning Rate: {best_hyperparams['learning_rate']:.6f}\")\n",
    "    print(f\"Batch Size: {best_hyperparams['batch_size']}\")\n",
    "    print(f\"Epochs: {best_hyperparams['epochs']}\")\n",
    "    print(f\"Best Validation Loss: {best_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48cd87c",
   "metadata": {},
   "source": [
    "# Task 2.1 Some theory\n",
    "\n",
    "Let's think about mutations and if the implementation in the current state can be optimized. \n",
    "\n",
    "- Is it useful, that mutation in late generations occur in the same rate and same \"brutality\" as in early generations? Should we address this? If yes, how? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef34ab",
   "metadata": {},
   "source": [
    "1) Adaptive Mutation Rate based on generations (inversely proportional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08dbf211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutation_rate = base_mutation_rate * (1 - generation / max_generations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e05bb",
   "metadata": {},
   "source": [
    "\n",
    "2) Gradual Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7339bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if random.random() < mutation_rate:\n",
    "#         new_params['hidden_units'] += random.randint(-8, 8)  # Adjust within Â±8\n",
    "#         new_params['hidden_units'] = max(4, min(new_params['hidden_units'], 128))  # Ensure bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44d539f",
   "metadata": {},
   "source": [
    "3) Hybrid approach (combination of 1 and 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2550526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if random.random() < mutation_rate:\n",
    "#     if random.random() < 0.5:\n",
    "#         new_params['hidden_units'] = random.randint(4, 128)  # Abrupt change\n",
    "#     else:\n",
    "#         new_params['hidden_units'] += random.randint(-8, 8)  # Small adjustment\n",
    "#         new_params['hidden_units'] = max(4, min(new_params['hidden_units'], 128))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b8667",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
