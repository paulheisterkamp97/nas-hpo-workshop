{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9068dc",
   "metadata": {},
   "source": [
    "### A regression problem\n",
    "\n",
    "We want to predict y for x in a sine function. We will use a neural network to predict y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc98a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Task: Create a synthetic regression dataset\n",
    "# Generate a sine wave with noise\n",
    "x = torch.linspace(0, 2 * torch.pi, 2000).view(-1, 1)  # 1000 samples\n",
    "y = torch.sin(x) + 0.1 * torch.randn(x.size())         # Add noise\n",
    "\n",
    "# Task: Split into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Wrap the datasets in PyTorch DataLoaders\n",
    "train_dataset = TensorDataset(torch.tensor(x_train).float(), torch.tensor(y_train).float())\n",
    "val_dataset = TensorDataset(torch.tensor(x_val).float(), torch.tensor(y_val).float())\n",
    "\n",
    "# Leave batch size for tuning later\n",
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df23203",
   "metadata": {},
   "source": [
    "#### This is our deep Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1664acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define a deeper feedforward neural network\n",
    "class DeeperNN(nn.Module):\n",
    "    def __init__(self, hidden_units=128):\n",
    "        super(DeeperNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(1, hidden_units)\n",
    "        self.hidden2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.hidden3 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.output = nn.Linear(hidden_units, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = torch.relu(self.hidden3(x))\n",
    "        return self.output(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = DeeperNN(hidden_units=128)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688d9f73",
   "metadata": {},
   "source": [
    "#### Here we define basic a training function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            predictions = model(batch_x)\n",
    "            val_loss += criterion(predictions, batch_y).item()\n",
    "    return val_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e26d1",
   "metadata": {},
   "source": [
    "And now onto our first task. \n",
    "\n",
    "# Task 1\n",
    "\n",
    "Expand the hyperparameters for Grid Search (directly below). \n",
    "The more values you add, the bigger the search space will be - this is usually good practice, as you don't want to get stuck in local minima but rather find the global minumum -> The hyperparameter combination with the least validation loss. Adjust the amount of epochs. \n",
    "\n",
    "\n",
    "![Local Minimum](https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Extrema_example_original.svg/500px-Extrema_example_original.svg.png)\n",
    "\n",
    "\n",
    "\n",
    "Run the cell after you expanded the hyperparameter sets by as many values as you think are enough. The training might take a few minutes. \n",
    "\n",
    "\n",
    "\n",
    "Compare the results with the Random Search results one block below Grid Search. What did you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8404857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for grid search\n",
    "hidden_units_grid = [128, 256]\n",
    "learning_rates = [1, 0.1]\n",
    "batch_sizes = [2, 4]\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d31739",
   "metadata": {},
   "source": [
    "Our Grid Search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb4e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Grid Search iterates over all possible hyperparameter combinations\n",
    "for hidden_units in hidden_units_grid:\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            \n",
    "            # Prepare DataLoader\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "            # Initialize model, criterion, and optimizer\n",
    "            model = DeeperNN(hidden_units=hidden_units)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            # Train the model and get validation loss\n",
    "            val_loss = train_model(model, criterion, optimizer, train_loader, val_loader, epochs)\n",
    "            print(f\"hidden_units={hidden_units}, lr={lr}, batch_size={batch_size}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "            # Track the best configuration\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_params = (hidden_units, lr, batch_size)\n",
    "\n",
    "print(f\"Best Grid Search Params: hidden_units={best_params[0]}, lr={best_params[1]}, batch_size={best_params[2]} with val_loss={best_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6bcfc",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "\n",
    "We will use the same parameters you defined in the first task. However we won't explore every possible combination, but take random combinations for a number of trials - you can tweak the number of trials if you like, however this is not necessary. \n",
    "\n",
    "# Task 1.1\n",
    "\n",
    "Implement a way to randomly chose values from the sets you defined above. \n",
    "\n",
    "Hint: Check the import statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63047146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Random Search\n",
    "num_trials = 10\n",
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "\n",
    "for _ in range(num_trials):\n",
    "    # Randomly sample hyperparameters TODO: Implement \n",
    "    hidden_units = something_random # from hidden_units_grid\n",
    "    lr = something_random # from learning_rates\n",
    "    batch_size = something_random # from batch_sizes\n",
    "\n",
    "    # Prepare DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = DeeperNN(hidden_units=hidden_units)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train the model and get validation loss\n",
    "    val_loss = train_model(model, criterion, optimizer, train_loader, val_loader, epochs)\n",
    "    print(f\"hidden_units={hidden_units}, lr={lr}, batch_size={batch_size}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "    # Track the best configuration\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_params = (hidden_units, lr, batch_size)\n",
    "\n",
    "print(f\"Best Random Search Params: hidden_units={best_params[0]}, lr={best_params[1]}, batch_size={best_params[2]} with val_loss={best_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dc04ba",
   "metadata": {},
   "source": [
    "#### What did you notice? What are key differences? \n",
    "\n",
    "Put your answers here. \n",
    "\n",
    "#### Even if results may be better by a small margin, what makes Grid Search unpractial compared to Random Search?\n",
    "\n",
    "Hint: Think about the relation between the number of Hyperparameters and the number of combinations that result from that. Especially for harder problems with more dimensions in the search space. \n",
    "\n",
    "Put your answers here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf3f41e",
   "metadata": {},
   "source": [
    "## Evolutionary Algorithms\n",
    "\n",
    "The following codeblock is a basic implementation of an Evolutionary Algorithm. Go over it and try to understand what happens. Remember what was told in the theoretical part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ae47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "def evaluate_model(model, train_loader, val_loader, learning_rate, epochs):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# Perform mutation on hyperparameters\n",
    "def mutate(params, mutation_rate=0.1):\n",
    "    new_params = params.copy()\n",
    "    if random.random() < mutation_rate:\n",
    "        new_params['hidden_units'] = random.randint(4, 128)  # Choose hidden units randomly between 4 and 128\n",
    "    if random.random() < mutation_rate:\n",
    "        new_params['learning_rate'] = random.uniform(0.0001, 1)  # Choose learning rate randomly in range\n",
    "    if random.random() < mutation_rate:\n",
    "        new_params['batch_size'] = random.randint(2, 128)  # Choose batch size between 2 and 128\n",
    "    if random.random() < mutation_rate:\n",
    "        new_params['epochs'] = random.randint(10, 100)  # Mutate epochs within a range\n",
    "    return new_params\n",
    "\n",
    "# Perform crossover between two sets of hyperparameters\n",
    "def crossover(parent1, parent2):\n",
    "    child = {}\n",
    "    for key in parent1:\n",
    "        child[key] = parent1[key] if random.random() > 0.5 else parent2[key]\n",
    "    return child\n",
    "\n",
    "# Main evolutionary algorithm\n",
    "def evolutionary_algorithm(num_generations=20, population_size=10, mutation_rate=0.5):\n",
    "    # Initialize population with random hyperparameters\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        params = {\n",
    "            'hidden_units': random.randint(4, 1024),  # Randomly chosen hidden_units\n",
    "            'learning_rate': random.uniform(0.0001, 1),  # Randomly chosen learning_rate\n",
    "            'batch_size': random.randint(2, 1048),  # Randomly chosen batch_size\n",
    "            'epochs': random.randint(1, 50)  # Randomly chosen epochs\n",
    "        }\n",
    "        population.append(params)\n",
    "\n",
    "    best_hyperparams = None\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for generation in range(num_generations):\n",
    "        # Evaluate fitness of each set of hyperparameters in the population\n",
    "        fitness = []\n",
    "        for params in population:\n",
    "            # Create DataLoaders with the specified batch size\n",
    "            train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=params['batch_size'])\n",
    "\n",
    "            # Initialize a new model with the current hyperparameters\n",
    "            model = DeeperNN(hidden_units=params['hidden_units'])\n",
    "\n",
    "            # Evaluate the model's validation loss\n",
    "            val_loss = evaluate_model(model, train_loader, val_loader, params['learning_rate'], params['epochs'])\n",
    "            fitness.append((val_loss, params))\n",
    "\n",
    "            # Update the best hyperparameters if the current model is better\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_hyperparams = params\n",
    "\n",
    "        # Sort population based on fitness (lower loss = better fitness)\n",
    "        fitness.sort(key=lambda x: x[0])\n",
    "        population = [individual[1] for individual in fitness]\n",
    "\n",
    "        # Print the best fitness in the current generation\n",
    "        print(f\"Generation {generation + 1}: Best Validation Loss = {fitness[0][0]:.6f}\")\n",
    "\n",
    "        # Elitism: Keep the top 2 hyperparameter sets unchanged\n",
    "        next_population = population[:2]\n",
    "\n",
    "        # Generate the rest of the population via crossover and mutation\n",
    "        while len(next_population) < population_size:\n",
    "            parent1 = random.choice(population[:population_size // 2])\n",
    "            parent2 = random.choice(population[:population_size // 2])\n",
    "            child = crossover(parent1, parent2)\n",
    "            child = mutate(child, mutation_rate=mutation_rate)  # Mutate the child's hyperparameters\n",
    "            next_population.append(child)\n",
    "\n",
    "        # Update the population\n",
    "        population = next_population\n",
    "\n",
    "    return best_hyperparams, best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e6160",
   "metadata": {},
   "source": [
    "# Task 2: Evolutionary Algorithms\n",
    "\n",
    "![Local Minimum](https://www.americanscientist.org/sites/americanscientist.org/files/20144141249210337-2014-05TechnologueFp170.jpg)\n",
    "\n",
    "Now this one is a bit more tricky! We now want to use an Evolutionary Algorithm to find good Hyperparameters. Remember the theory behind selections. \n",
    "\n",
    "\n",
    "Run the algorithm with the default values first. What do you notice? \n",
    "\n",
    "If your machine is not that powerful, training might take more than a few minutes. In that case you can stop the calculations after few generations. \n",
    "\n",
    "- Remember what the mutation rate does? Maybe try tweaking this value first in a useful manner. Hint: Check the mutation function. Write down why the default value is maybe not the best fit. \n",
    "\n",
    "- Maybe you want to check the population size next. Check how the algorithm choses the parents to influence the next generations population in the code above and tweak the value. So, why is the default population size of 4 maybe not the best choice? \n",
    "\n",
    "- Alright, step by step in the right direction. Maybe it makes sense to increase the number of generations? Check the comic in respect to the randomness in mutations to understand why ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97193b",
   "metadata": {},
   "source": [
    "Here is space for your notes and answers.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c29f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evolutionary algorithm\n",
    "if __name__ == \"__main__\":\n",
    "    num_generations = 5\n",
    "    population_size = 4\n",
    "    mutation_rate = 0.8\n",
    "\n",
    "    best_hyperparams, best_loss = evolutionary_algorithm(\n",
    "        num_generations=num_generations, \n",
    "        population_size=population_size,\n",
    "        mutation_rate=mutation_rate\n",
    "    )\n",
    "\n",
    "    # Print the best hyperparameters and corresponding loss\n",
    "    print(\"\\nBest Hyperparameters:\")\n",
    "    print(f\"Hidden Units: {best_hyperparams['hidden_units']}\")\n",
    "    print(f\"Learning Rate: {best_hyperparams['learning_rate']:.6f}\")\n",
    "    print(f\"Batch Size: {best_hyperparams['batch_size']}\")\n",
    "    print(f\"Epochs: {best_hyperparams['epochs']}\")\n",
    "    print(f\"Best Validation Loss: {best_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48cd87c",
   "metadata": {},
   "source": [
    "# Task 2.1 Some theory\n",
    "\n",
    "Let's think about mutations and whether the implementation in the current state can be optimized. \n",
    "\n",
    "Hint: Is it useful, that mutation in late generations occur in the same rate and same \"brutality\" as in early generations? Should we address this? If yes, how? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
